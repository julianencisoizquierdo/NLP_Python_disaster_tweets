{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7462099,
          "sourceType": "datasetVersion",
          "datasetId": 4343103
        },
        {
          "sourceId": 7490938,
          "sourceType": "datasetVersion",
          "datasetId": 4361383
        }
      ],
      "dockerImageVersionId": 30635,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julianencisoizquierdo/NNP-Disaster-Tweets/blob/main/NLP_Disaster_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'disaster-tweets:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4343103%2F7462099%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240204%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240204T175141Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dbc8ebc6d766f2daf67553cd3f40af2a8291a54fc589622473f6059309a9b90bb9bbb65156ebd7ee37f3deb77eded475e24ab40a9c4c3e8fead59f01f1fed4930b55f1d05f0aefdfa9970ab12b06235077718266652c2a7686ef1b965fe3b57894241c6717087c4cb0820fd66b2dec361252a1eac6ee14bf813b1176c49414edb6464577b8cd455fb86859480b513a09c47eb41f4be6ab9e479173fbaeecad18948390d95391ae0d20fcf5bdc91d2747a361fda528c81f2995d5289618beb00f84d83003607e847fc4ccb511f69a476e80d7318d6b31c384d22f233d5cb40edceebcb52f7386454c648e8f966f8bcf7d24dec612ce0cb50780e4fecf7d7395421,frequency-dictionary:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4361383%2F7490938%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240204%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240204T175141Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0a5322989c9ed5cdb5113a2e4fccf24a18e40799f288e7474daebf18021ac1ce131fd3d13e95e92ed9e8627dca035e97ee36b9bf179dbaaf0a2209aa2804f773120e3a5f70192e0f6ba3bad3efda2adf3c31100baa15d04b964f41748ab8a83b681a4aa2ff331b916d901b1351a4fc7c809134b01e92032ea6ddf7a3d80c9cfa61ecfa52c15a06013ca72aadf17a99d123f62a507fe63b26c19c7c57d9de331fd6bef5703591462cadc11dd0c2e657605e8e928b426c2594c58b73edee4017229558d2fe08ce7fcaad83b8d09919d6de3eee32e580cb046f0a07c2e23c718a77326286b0787400d75ff93295727ead564f76d322e9a37881854994fe8f0b7c00'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "XJIGhNhwP2sI"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras_core as keras\n",
        "import keras_nlp\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.options.display.max_columns = 30\n",
        "pd.options.display.max_rows = 60\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "pd.options.display.max_colwidth = 200\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"KerasNLP version:\", keras_nlp.__version__)\n",
        "\n",
        "\n",
        "\n",
        "df_train = pd.read_csv('/kaggle/input/disaster-tweets/train.csv')\n",
        "df_test = pd.read_csv('/kaggle/input/disaster-tweets/test.csv')\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "print('Training Set Shape = {}'.format(df_train.shape))\n",
        "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
        "print('Test Set Shape = {}'.format(df_test.shape))\n",
        "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))\n",
        "\n",
        "\n",
        "# DATA EXPLORATION\n",
        "print('\\nInfo about the training set:\\n', df_train.info())\n",
        "print('\\nInfo about the test set:\\n', df_test.info())\n",
        "\n",
        "\n",
        "# There are missing values in location and keyboard\n",
        "per_keyword = df_train['keyword'].isna().sum() / df_train['keyword'].count() * 100\n",
        "print(f\"The percentage of NaN values in 'keyword' column in the training set is: {per_keyword:.2f}%\")\n",
        "\n",
        "per_loc = df_train['location'].isna().sum() / df_train['location'].count() * 100\n",
        "print(f\"The percentage of NaN values in 'location' column in the training set is: {per_loc:.2f}%\")\n",
        "\n",
        "per_keyword_test = df_test['keyword'].isna().sum() / df_test['keyword'].count() * 100\n",
        "print(f\"The percentage of NaN values in 'keyword' column in the test set is: {per_keyword_test:.2f}%\")\n",
        "\n",
        "per_loc_test = df_test['location'].isna().sum() / df_test['location'].count() * 100\n",
        "print(f\"The percentage of NaN values in 'location' column in the test set is: {per_loc_test:.2f}%\")\n",
        "\n",
        "\n",
        "# The amount of missing values are the more or less the same in both the train and the test dataset\n",
        "\n",
        "df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\n",
        "df_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n",
        "\n",
        "print(\"Train Length Stat\")\n",
        "print(df_train[\"length\"].describe())\n",
        "print()\n",
        "\n",
        "print(\"Test Length Stat\")\n",
        "print(df_test[\"length\"].describe())\n",
        "\n",
        "\n",
        "df_train.head(40)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "8_FbARt2P2sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Processing**"
      ],
      "metadata": {
        "id": "8W1MWirtP2sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string\n",
        "\n",
        "!pip install symspellpy\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "import spacy\n",
        "import os\n",
        "\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Lowercasing\n",
        "\n",
        "df_train['keyword'] = df_train['keyword'].apply(lambda x: str.lower(x) if pd.isna(x) != True else x)\n",
        "df_train['location'] = df_train['location'].apply(lambda x: str.lower(x) if pd.isna(x) != True else x)\n",
        "df_train['text'] = df_train['text'].apply(lambda x: str.lower(x))\n",
        "\n",
        "df_test['keyword'] = df_test['keyword'].apply(lambda x: str.lower(x) if pd.isna(x) != True else x)\n",
        "df_test['location'] = df_test['location'].apply(lambda x: str.lower(x) if pd.isna(x) != True else x)\n",
        "df_test['text'] = df_test['text'].apply(lambda x: str.lower(x))\n",
        "\n",
        "\n",
        "\n",
        "# Punctuation removal\n",
        "\n",
        "def remove_ent(text):\n",
        "    ent_prefixes = ['@', '#']\n",
        "    for separator in string.punctuation:\n",
        "        if separator not in ent_prefixes:\n",
        "            text = text.replace(separator, ' ')\n",
        "    words = []\n",
        "    for word in text.split():\n",
        "        word = word.strip()\n",
        "        if word:\n",
        "            if word[0] not in ent_prefixes:\n",
        "                words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "df_train['keyword'] = df_train['keyword'].apply(lambda x: remove_ent(x) if pd.isna(x) != True else x)\n",
        "df_train['location'] = df_train['location'].apply(lambda x: remove_ent(x) if pd.isna(x) != True else x)\n",
        "df_train['text'] = df_train['text'].apply(lambda x: remove_ent(x))\n",
        "\n",
        "df_test['keyword'] = df_test['keyword'].apply(lambda x: remove_ent(x) if pd.isna(x) != True else x)\n",
        "df_test['location'] = df_test['location'].apply(lambda x: remove_ent(x) if pd.isna(x) != True else x)\n",
        "df_test['text'] = df_test['text'].apply(lambda x: remove_ent(x))\n",
        "\n",
        "\n",
        "# Spelling Correction\n",
        "\n",
        "sym_spell = SymSpell()\n",
        "\n",
        "dictionary_path = '/kaggle/input/frequency-dictionary/frequency_dictionary_en_82_765.txt'\n",
        "\n",
        "sym_spell.load_dictionary(dictionary_path, 0, 1)\n",
        "\n",
        "def spelling_correction(sent):\n",
        "    doc_w_cor_spelling = []\n",
        "    for tok in sent.split(' '):\n",
        "\n",
        "        x = sym_spell.lookup(tok, Verbosity.CLOSEST, max_edit_distance = 2, include_unknown=True)[0].__str__()\n",
        "        y = x.split(',')[0]\n",
        "        doc_w_cor_spelling.append(y)\n",
        "\n",
        "    return \" \".join(doc_w_cor_spelling)\n",
        "\n",
        "df_train['keyword'] = df_train['keyword'].apply(lambda x: spelling_correction(x) if pd.isna(x) != True else x)\n",
        "df_train['location'] = df_train['location'].apply(lambda x: spelling_correction(x) if pd.isna(x) != True else x)\n",
        "df_train['text'] = df_train['text'].apply(lambda x: spelling_correction(x))\n",
        "\n",
        "df_test['keyword'] = df_test['keyword'].apply(lambda x: spelling_correction(x) if pd.isna(x) != True else x)\n",
        "df_test['location'] = df_test['location'].apply(lambda x: spelling_correction(x) if pd.isna(x) != True else x)\n",
        "df_test['text'] = df_test['text'].apply(lambda x: spelling_correction(x))\n",
        "\n",
        "df_train.head(40)\n",
        "\n",
        "\n",
        "\n",
        "# Keyword extraction\n",
        "\n",
        "os.system('python -m spacy download en')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_keywords(nlp=nlp, doc=\"\", no_of_keywords=5, model=model):\n",
        "\n",
        "    doc = doc.lower()\n",
        "    doc = re.sub(r'?:\\@|http?\\://|https?\\://|www\\S+', ' ', doc)\n",
        "    doc = re.sub(r'[^\\w\\s]', ' ', doc)\n",
        "    doc = re.sub(' \\d+', ' ', doc)\n",
        "\n",
        "    doc_ = nlp(doc)\n",
        "\n",
        "\n",
        "    pos_tag = ['VERB', 'NOUN', 'AJD', 'PROPN']\n",
        "    result = []\n",
        "\n",
        "    for token in doc_:\n",
        "        if (token.pos_ in pos_tag):\n",
        "            result.append(token.text)\n",
        "\n",
        "doc_embedding = model.encode([doc])\n",
        "results_embeddings = model.encode(result)\n",
        "\n",
        "\n",
        "distances = cosine_similarity(doc_embedding, results_embeddings)\n",
        "\n",
        "\n",
        "keywords = [result[index] for index in distances.argsort()[0][-no_of_keywords:]]\n",
        "\n",
        "return keywords\n",
        "\n",
        "# https://medium.com/analytics-vidhya/introduction-to-nlp-with-disaster-tweets-3b672a75748c"
      ],
      "metadata": {
        "trusted": true,
        "id": "MsGIIA6GP2sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pd.read_csv('/kaggle/input/disaster-tweets/sample_submission.csv')\n",
        "\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "test.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-23T19:42:07.277935Z",
          "iopub.execute_input": "2024-01-23T19:42:07.278313Z",
          "iopub.status.idle": "2024-01-23T19:42:07.311216Z",
          "shell.execute_reply.started": "2024-01-23T19:42:07.278283Z",
          "shell.execute_reply": "2024-01-23T19:42:07.310386Z"
        },
        "trusted": true,
        "id": "KHP7M188P2sR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}